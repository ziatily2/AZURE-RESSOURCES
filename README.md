# ğŸš€ Azure Data Engineering Pipeline

## ğŸ“Œ Project Overview
This project demonstrates an **end-to-end data pipeline** leveraging **Azure** services for data ingestion, transformation, and storage. It follows best practices for **incremental data processing** and **dimensional modeling** to build a scalable data architecture.

## ğŸ”° Tech Stack
âœ… **Azure Data Factory (ADF)** - Orchestration & Data Ingestion  
âœ… **Databricks with Unity Catalog** - Data Transformation & Governance  
âœ… **Apache Spark** - Distributed Data Processing  
âœ… **Delta Lake** - Optimized Data Storage & Versioning  
âœ… **Azure Data Lake** - Scalable Storage Layer  
âœ… **Azure SQL Database** - Source System  
âœ… **GitHub** - Version Control & Collaboration  

## ğŸ”° Architecture
âœ… **Medallion Architecture** - Implementing **Bronze, Silver, Gold layers** for better data management  

## ğŸ”° Complex Real-Time Scenarios
âœ… **Incremental Data Loading** - Handling new data efficiently  
âœ… **Dimensional Data Modeling** - Structuring data for analytics  
âœ… **STAR Schema Design** - Optimizing for performance  
âœ… **Slowly Changing Dimensions (SCDs)** - Managing historical data changes  

## ğŸ”„ Pipeline Workflow
1ï¸âƒ£ **Data Ingestion**: Extracting data from **Azure SQL Database** using **ADF Copy Activity**
2ï¸âƒ£ **Bronze Layer (Raw Storage)**: Storing extracted data in **Azure Data Lake Gen2**
3ï¸âƒ£ **Incremental Processing**: Applying a **Watermark Strategy** to load only new data
4ï¸âƒ£ **Silver Layer (Cleansed Data)**: Processing and cleaning data with **Databricks Notebooks**
5ï¸âƒ£ **Gold Layer (Analytics Ready Data)**: Creating structured tables such as **Dim_branch, Dim_date, Dim_model, Fact_sales**
6ï¸âƒ£ **Serving Layer**: Storing transformed data in **Delta Lake** for optimized querying


