# ğŸš€ Azure Data Engineering Pipeline

## ğŸ“Œ Project Overview
This project demonstrates an **end-to-end data pipeline** leveraging **Azure** services for data ingestion, transformation, and storage. It follows best practices for **incremental data processing** and **dimensional modeling** to build a scalable data architecture.

## ğŸ”° Tech Stack
âœ… **Azure Data Factory (ADF)** - Orchestration & Data Ingestion  
âœ… **Databricks with Unity Catalog** - Data Transformation & Governance  
âœ… **Apache Spark** - Distributed Data Processing  
âœ… **Delta Lake** - Optimized Data Storage & Versioning  
âœ… **Azure Data Lake** - Scalable Storage Layer  
âœ… **Azure SQL Database** - Source System  
âœ… **GitHub** - Version Control & Collaboration  

## ğŸ”° Architecture
âœ… **Medallion Architecture** - Implementing **Bronze, Silver, Gold layers** for better data management  

## ğŸ”° Complex Real-Time Scenarios
âœ… **Incremental Data Loading** - Handling new data efficiently  
âœ… **Dimensional Data Modeling** - Structuring data for analytics  
âœ… **STAR Schema Design** - Optimizing for performance  
âœ… **Slowly Changing Dimensions (SCDs)** - Managing historical data changes  

## ğŸ”„ Pipeline Workflow
1ï¸âƒ£ **Data Ingestion**: Extracting data from **Azure SQL Database** using **ADF Copy Activity**
2ï¸âƒ£ **Bronze Layer (Raw Storage)**: Storing extracted data in **Azure Data Lake Gen2**
3ï¸âƒ£ **Incremental Processing**: Applying a **Watermark Strategy** to load only new data
4ï¸âƒ£ **Silver Layer (Cleansed Data)**: Processing and cleaning data with **Databricks Notebooks**
5ï¸âƒ£ **Gold Layer (Analytics Ready Data)**: Creating structured tables such as **Dim_branch, Dim_date, Dim_model, Fact_sales**
6ï¸âƒ£ **Serving Layer**: Storing transformed data in **Delta Lake** for optimized querying

## ğŸ“‚ Repository Structure
```
ğŸ“¦ AZURE-RESSOURCES
 â”£ ğŸ“‚ notebooks
 â”ƒ â”£ ğŸ“œ silver_notebook.ipynb  # Data transformation logic
 â”ƒ â”£ ğŸ“œ gold_dim_date.ipynb    # Dimensional modeling (Date dimension)
 â”ƒ â”£ ğŸ“œ gold_fact_sales.ipynb  # Fact table processing (Sales data)
 â”£ ğŸ“‚ pipelines
 â”ƒ â”£ ğŸ“œ incremental_data_pipeline.json  # ADF Pipeline Configuration
 â”£ ğŸ“œ README.md  # Project Documentation
```

## ğŸ“· Project Screenshots
âœ… **Azure Data Factory Pipeline** - Incremental Data Loading & Orchestration
âœ… **Databricks Job Execution** - ETL Pipeline Execution & Dimensional Modeling

## ğŸš€ Getting Started
### 1ï¸âƒ£ Clone Repository
```bash
git clone https://github.com/ziatily2/AZURE-RESSOURCES.git
```
### 2ï¸âƒ£ Deploy Azure Data Factory Pipeline
- Import **incremental_data_pipeline.json** into Azure Data Factory.
- Configure **SQL Database** and **Data Lake Gen2** connections.

### 3ï¸âƒ£ Deploy Databricks Notebooks
- Upload notebooks to **Databricks Workspace**.
- Schedule jobs using **Databricks Workflows**.

## ğŸ’¡ Future Enhancements
- **Integrate Advanced Unity Catalog Features** for enhanced data governance and access control.
- **Optimize Databricks Workflows** for better performance and cost efficiency.
- **Implement CI/CD Pipelines** using GitHub Actions for automation.

## ğŸ”— Connect with Me
Iâ€™d love to collaborate and hear your feedback! ğŸš€
